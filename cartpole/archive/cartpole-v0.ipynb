{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gym\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "observation state space consists of cartpole position, cartpole velocity, pole angle, pole velocity\n",
      "lower_bound on observation state spaces:  [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "upper_bound on observation state spaces:  [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "action space consists of 0-push left, 1-push right\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(\"observation state space consists of cartpole position, cartpole velocity, pole angle, pole velocity\")\n",
    "if(env.observation_space.is_bounded):\n",
    "    print(\"lower_bound on observation state spaces: \", env.observation_space.low)\n",
    "    print(\"upper_bound on observation state spaces: \", env.observation_space.high)\n",
    "print(\"action space consists of 0-push left, 1-push right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 4 discrete states and 2 discrete actions.\n",
    "# Choose discretization criteria for the states\n",
    "# Considering only pole angle and position\n",
    "buckets=(1, 1, 6, 12,)\n",
    "def discretize(obs):\n",
    "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon value is 0.949\n",
      "alhpa value is 0.101\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 14.0 ticks.\n",
      "Epsilon value is 0.8489999999999999\n",
      "alhpa value is 0.2010000000000001\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 20.128712871287128 ticks.\n",
      "Epsilon value is 0.7489999999999998\n",
      "alhpa value is 0.30100000000000016\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 20.412935323383085 ticks.\n",
      "Epsilon value is 0.6489999999999997\n",
      "alhpa value is 0.40100000000000025\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 19.398671096345517 ticks.\n",
      "Epsilon value is 0.5489999999999996\n",
      "alhpa value is 0.5010000000000003\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 18.306733167082296 ticks.\n",
      "Epsilon value is 0.4489999999999995\n",
      "alhpa value is 0.6010000000000004\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 17.461077844311376 ticks.\n",
      "Epsilon value is 0.3489999999999994\n",
      "alhpa value is 0.7010000000000005\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 16.594009983361065 ticks.\n",
      "Epsilon value is 0.24899999999999933\n",
      "alhpa value is 0.8010000000000006\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 15.857346647646219 ticks.\n",
      "Epsilon value is 0.14899999999999924\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 15.255930087390762 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 14.658157602663707 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 14.171828171828173 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 13.800181653042689 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 13.480432972522898 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1300] - Mean survival time over last 100 episodes was 13.20445810914681 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1400] - Mean survival time over last 100 episodes was 12.972876516773733 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1500] - Mean survival time over last 100 episodes was 12.772151898734178 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1600] - Mean survival time over last 100 episodes was 12.580262336039976 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1700] - Mean survival time over last 100 episodes was 12.423280423280424 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1800] - Mean survival time over last 100 episodes was 12.284286507495835 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 1900] - Mean survival time over last 100 episodes was 12.156233561283535 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2000] - Mean survival time over last 100 episodes was 12.034982508745626 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2100] - Mean survival time over last 100 episodes was 11.940028557829605 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2200] - Mean survival time over last 100 episodes was 11.850522489777374 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2300] - Mean survival time over last 100 episodes was 11.77748804867449 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2400] - Mean survival time over last 100 episodes was 11.70387338608913 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2500] - Mean survival time over last 100 episodes was 11.63374650139944 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2600] - Mean survival time over last 100 episodes was 11.574009996155326 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2700] - Mean survival time over last 100 episodes was 11.505738615327656 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2800] - Mean survival time over last 100 episodes was 11.442699036058551 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 2900] - Mean survival time over last 100 episodes was 11.38503964150293 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3000] - Mean survival time over last 100 episodes was 11.335888037320894 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3100] - Mean survival time over last 100 episodes was 11.292163818123186 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3200] - Mean survival time over last 100 episodes was 11.248359887535145 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3300] - Mean survival time over last 100 episodes was 11.206301120872462 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3400] - Mean survival time over last 100 episodes was 11.165833578359306 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3500] - Mean survival time over last 100 episodes was 11.131105398457583 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3600] - Mean survival time over last 100 episodes was 11.096362121632879 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3700] - Mean survival time over last 100 episodes was 11.065928127533098 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3800] - Mean survival time over last 100 episodes was 11.035253880557748 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 3900] - Mean survival time over last 100 episodes was 11.002050756216354 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4000] - Mean survival time over last 100 episodes was 10.972006998250437 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4100] - Mean survival time over last 100 episodes was 10.942940746159474 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4200] - Mean survival time over last 100 episodes was 10.919066888835992 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4300] - Mean survival time over last 100 episodes was 10.89351313647989 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4400] - Mean survival time over last 100 episodes was 10.86821177005226 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4500] - Mean survival time over last 100 episodes was 10.850477671628527 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4600] - Mean survival time over last 100 episodes was 10.827646163877418 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4700] - Mean survival time over last 100 episodes was 10.8085513720485 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4800] - Mean survival time over last 100 episodes was 10.794001249739637 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 4900] - Mean survival time over last 100 episodes was 10.775351968985921 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5000] - Mean survival time over last 100 episodes was 10.754449110177964 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5100] - Mean survival time over last 100 episodes was 10.742207410311703 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5200] - Mean survival time over last 100 episodes was 10.724091520861373 ticks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5300] - Mean survival time over last 100 episodes was 10.708922844746274 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5400] - Mean survival time over last 100 episodes was 10.698204036289576 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5500] - Mean survival time over last 100 episodes was 10.68314851845119 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5600] - Mean survival time over last 100 episodes was 10.667380824852705 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5700] - Mean survival time over last 100 episodes was 10.653394141378705 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5800] - Mean survival time over last 100 episodes was 10.639717290122393 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 5900] - Mean survival time over last 100 episodes was 10.627351296390442 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6000] - Mean survival time over last 100 episodes was 10.618230294950841 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6100] - Mean survival time over last 100 episodes was 10.606294050155713 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6200] - Mean survival time over last 100 episodes was 10.594581519109822 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6300] - Mean survival time over last 100 episodes was 10.584034280272972 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6400] - Mean survival time over last 100 episodes was 10.571473207311358 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6500] - Mean survival time over last 100 episodes was 10.562067374250116 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6600] - Mean survival time over last 100 episodes was 10.550068171489169 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6700] - Mean survival time over last 100 episodes was 10.537680943142815 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6800] - Mean survival time over last 100 episodes was 10.52801058667843 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 6900] - Mean survival time over last 100 episodes was 10.519055209389943 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7000] - Mean survival time over last 100 episodes was 10.51249821454078 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7100] - Mean survival time over last 100 episodes was 10.502746092099704 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7200] - Mean survival time over last 100 episodes was 10.494098041938619 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7300] - Mean survival time over last 100 episodes was 10.486919600054787 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7400] - Mean survival time over last 100 episodes was 10.476962572625322 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7500] - Mean survival time over last 100 episodes was 10.46647113718171 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7600] - Mean survival time over last 100 episodes was 10.458886988554138 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7700] - Mean survival time over last 100 episodes was 10.453058044409817 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7800] - Mean survival time over last 100 episodes was 10.446353031662607 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 7900] - Mean survival time over last 100 episodes was 10.439058347044678 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8000] - Mean survival time over last 100 episodes was 10.432070991126109 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8100] - Mean survival time over last 100 episodes was 10.42328107641032 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8200] - Mean survival time over last 100 episodes was 10.41641263260578 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8300] - Mean survival time over last 100 episodes was 10.408384531984098 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8400] - Mean survival time over last 100 episodes was 10.402809189382216 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8500] - Mean survival time over last 100 episodes was 10.395835784025408 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8600] - Mean survival time over last 100 episodes was 10.39297756074875 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8700] - Mean survival time over last 100 episodes was 10.387426732559476 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8800] - Mean survival time over last 100 episodes was 10.38109305760709 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 8900] - Mean survival time over last 100 episodes was 10.375351084147848 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9000] - Mean survival time over last 100 episodes was 10.369514498389067 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9100] - Mean survival time over last 100 episodes was 10.365783979782442 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9200] - Mean survival time over last 100 episodes was 10.361373763721335 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9300] - Mean survival time over last 100 episodes was 10.355875712289 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9400] - Mean survival time over last 100 episodes was 10.34911179661738 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9500] - Mean survival time over last 100 episodes was 10.343753289127461 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9600] - Mean survival time over last 100 episodes was 10.339860431205082 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9700] - Mean survival time over last 100 episodes was 10.336769405215957 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9800] - Mean survival time over last 100 episodes was 10.330884603611876 ticks.\n",
      "Epsilon value is 0.0999999999999992\n",
      "alhpa value is 0.9000000000000007\n",
      "[Episode 9900] - Mean survival time over last 100 episodes was 10.327138672861327 ticks.\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning approach\n",
    "episodes = 10000\n",
    "done = False\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.95\n",
    "\n",
    "reward_history = []\n",
    "average_reward_history = []\n",
    "counter = 0\n",
    "\n",
    "ns0 = int(10*(env.observation_space.high[0] - env.observation_space.low[0])) + 1\n",
    "ns1 = int(10*(env.observation_space.high[2] - env.observation_space.low[2])) + 1\n",
    "# q_table = np.random.uniform(low=-1, high=1, size=(ns0, ns1, env.action_space.n))\n",
    "# q_table = np.zeros((ns0, ns1, env.action_space.n))\n",
    "Q = np.zeros(buckets + (env.action_space.n,))\n",
    "\n",
    "# for every episode\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # reset environment\n",
    "    current_state = discretize(env.reset())\n",
    "#     q_current_state = [int(10*(current_state[0]-env.observation_space.low[0])), int(10*(current_state[2]-env.observation_space.low[2]))]\n",
    "    \n",
    "    # decay epsilon\n",
    "    if epsilon > 0.1:\n",
    "        epsilon = epsilon - 10/episodes\n",
    "    if alpha < 0.9:\n",
    "        alpha = alpha + 10/episodes\n",
    "    \n",
    "    # initialize any other value if needed\n",
    "    done = False\n",
    "    episodic_reward = 0\n",
    "\n",
    "    # until episode ends\n",
    "    while(not done):\n",
    "        \n",
    "        #choose when to render\n",
    "#         if(episodes%10 == 9):\n",
    "#             env.render()\n",
    "            \n",
    "        # choose an action to perform\n",
    "        # epsilon-greedy strategy\n",
    "        if(np.random.random() < epsilon): # exploration\n",
    "            action = env.action_space.sample()\n",
    "        else:                            # exploitation\n",
    "#             action = np.argmax(q_table[q_current_state[0], q_current_state[1]])\n",
    "            action = np.argmax(Q[current_state])\n",
    "                \n",
    "        \n",
    "        # execute the action\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "#         q_next_state = [int(10*(obs[0]-env.observation_space.low[0])), int(10*(obs[2]-env.observation_space.low[2]))]\n",
    "        next_state = discretize(obs)\n",
    "        \n",
    "        # update Q table based on this action\n",
    "#         q_table[q_current_state[0], q_current_state[1], action] = alpha*q_table[q_current_state[0], q_current_state[1], action] + (1-alpha)*(reward + gamma*np.argmax(q_table[q_next_state[0], q_next_state[1]]))\n",
    "        Q[current_state] = alpha*Q[current_state] + (1-alpha)*(reward + gamma*np.argmax(Q[next_state]))\n",
    "        \n",
    "        # next iteration preparation\n",
    "        current_state = next_state\n",
    "        counter += 1\n",
    "        \n",
    "        episodic_reward += reward\n",
    "        \n",
    "    reward_history.append(episodic_reward)\n",
    "    average_reward_history.append(np.mean(reward_history))\n",
    "    average_reward = np.mean(reward_history)\n",
    "    \n",
    "    if(average_reward>=195 and episode>=100):\n",
    "        print('Ran {} episodes. Solved after {} trials ✔'.format(episode, episode - 100))\n",
    "        break\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print('Epsilon value is {}'.format(epsilon))\n",
    "        print('alhpa value is {}'.format(alpha))\n",
    "        print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(episode, average_reward))\n",
    "            \n",
    "#             if(counter%100==99):\n",
    "#                 fig = plt.figure(1)\n",
    "#                 plt.clf()\n",
    "#                 plt.subplot(121)\n",
    "#                 plt.plot(reward_history,'r')\n",
    "#                 plt.xlabel('Episode')\n",
    "#                 plt.ylabel('Reward')\n",
    "#                 plt.title('Reward Per Episode')\n",
    "#                 plt.subplot(122)\n",
    "#                 plt.plot(average_reward_history, 'b')\n",
    "#                 plt.xlabel('Episode')\n",
    "#                 plt.ylabel('average_reward')\n",
    "#                 plt.title('average_reward plot')\n",
    "#                 plt.pause(0.01)\n",
    "#                 fig.canvas.draw()\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 12, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1,1,6,12,) + (env.action_space.n,)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 17.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 40.75 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 150.04 ticks.\n",
      "Ran 241 episodes. Solved after 141 trials ✔\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class QCartPoleSolver():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=195, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, quiet=False, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solver = QCartPoleSolver()\n",
    "    solver.run()\n",
    "    # gym.upload('tmp/cartpole-1', api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
